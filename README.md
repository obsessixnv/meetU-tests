# Bot Response Evaluation

This project evaluates the responses of various chatbots using several text similarity metrics: Cosine Similarity, ROUGE Recall, and BLEU Recall. The evaluation is performed on responses stored in a CSV file, which contains target responses and the responses generated by the bots.

## Code Overview

The code performs the following tasks:

1. **Load Data**: It reads a CSV file containing the target and bot responses.
2. **Calculate Similarity Metrics**:
   - **Cosine Similarity**: Measures the cosine of the angle between two non-zero vectors, indicating how similar the responses are.
   - **ROUGE Recall**: A metric commonly used for evaluating machine-generated text, particularly for summarization.
   - **BLEU Recall**: Used for measuring the quality of text which has been machine-translated from one language to another.
3. **Visualization**: It generates plots to visually compare the performance of the chatbots across the different metrics.
4. **Average Calculation**: It calculates and displays the average scores for each chatbot across the three metrics.

## Requirements

Make sure to install the required libraries:

```bash
pip install -r requirements.txt
```
# Chatbot Performance Analysis

## How to Use
1. Place your `bot_responses.csv` file in the same directory as the script.
2. Run the script to see the results and plots generated.

## Results
The following average scores were obtained for each bot:

- **Bot: Bella**
  - Average Cosine Similarity: 0.3265
  - Average ROUGE Recall: 0.0476
  - Average BLEU Recall: 0.0575

- **Bot: Mia**
  - Average Cosine Similarity: 0.3693
  - Average ROUGE Recall: 0.0605
  - Average BLEU Recall: 0.0687

- **Bot: Mike**
  - Average Cosine Similarity: 0.2137
  - Average ROUGE Recall: 0.0476
  - Average BLEU Recall: 0.0387

- **Bot: Olivia**
  - Average Cosine Similarity: 0.5097
  - Average ROUGE Recall: 0.0833
  - Average BLEU Recall: 0.2766

## Analysis
- **Cosine Similarity:** Olivia showed the highest cosine similarity score, indicating that its responses are the most similar to the target responses. Mike had the lowest score, suggesting its responses diverge more from the target.

- **ROUGE Recall:** Again, Olivia outperformed the other bots, indicating a better overlap with the target responses. Mike had the lowest ROUGE recall, similar to its cosine similarity performance.

- **BLEU Recall:** Olivia had the highest BLEU score, suggesting it generated responses that closely matched the target responses in terms of word choice and order. Mike, once more, had the lowest score.

Overall, Olivia consistently performed the best across all metrics, while Mike lagged behind.

## Plots
Insert your plots here.

## Conclusion
This analysis provides insights into the performance of different chatbots. Further improvements can be made by refining the models or enhancing the dataset to boost the quality of the responses.
